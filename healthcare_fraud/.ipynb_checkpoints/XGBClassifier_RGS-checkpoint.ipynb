{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f07595",
   "metadata": {},
   "source": [
    "# XGBoost with Random Grid Search\n",
    "### XGBClassifier (Sklearn API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae68ec",
   "metadata": {},
   "source": [
    "### No need xgboost.DMatrix (light gbm matrix format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccd39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "SEED = 123\n",
    "NTHREAD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_grid(train, valid, mono_constraints=None, gs_params=None, early_stopping_rounds=None, n_models=None,\n",
    "             ntree=None, verbose=None, seed=None):\n",
    "    \n",
    "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
    "\n",
    "    :param dtrain: Training data in LightSVM format.\n",
    "    :param dvalid: Validation data in LightSVM format.\n",
    "    :param mono_constraints: User-supplied monotonicity constraints.\n",
    "    :param gs_params: Dictionary of lists of potential XGBoost parameters over which to search.\n",
    "    :param n_models: Number of random models to evaluate.\n",
    "    :param ntree: Number of trees in XGBoost model.\n",
    "    :param early_stopping_rounds: XGBoost early stopping rounds.\n",
    "    :param verbose: Whether to display training iterations, default False.\n",
    "    :param seed: Random seed for better interpretability.\n",
    "    :return: Best candidate model from random grid search.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cartesian product of gs_params\n",
    "    keys, values = zip(*gs_params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # preserve exact reproducibility for this function\n",
    "    np.random.seed(SEED) \n",
    "    \n",
    "    # select randomly from cartesian product space\n",
    "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
    "\n",
    "    # set global params for objective,  etc.\n",
    "    params = {'booster': 'gbtree',\n",
    "              'n_jobs': NTHREAD,\n",
    "              'objective': 'binary:logistic', \n",
    "              'n_estimators':ntree,\n",
    "              'seed': SEED}\n",
    "\n",
    "    # init grid search loop\n",
    "    best_candidate = None\n",
    "    best_score = 0\n",
    "\n",
    "    # grid search loop\n",
    "    for i, exp in enumerate(selected_experiments):\n",
    "\n",
    "        params.update(experiments[exp])  # override global params with current grid run params\n",
    "\n",
    "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
    "        print('Training with parameters:', params)\n",
    "\n",
    "        # train on current params\n",
    "        watchlist = [(train[X_var], train[y_var]), (valid[X_var], valid[y_var])]\n",
    "        \n",
    "        if mono_constraints is not None:\n",
    "            params['monotone_constraints'] = mono_constraints\n",
    "        \n",
    "        model = XGBClassifier(**params\n",
    "                             )   \n",
    "        \n",
    "        candidate = model.fit(train[X_var], train[y_var], \n",
    "                              early_stopping_rounds = early_stopping_rounds, \n",
    "                              eval_set = watchlist, \n",
    "                              eval_metric='auc',\n",
    "                              verbose=verbose)\n",
    "\n",
    "        # determine if current model is better than previous best\n",
    "        eval_result = candidate.eval_result()\n",
    "        eval_od = list(eval_result.values())\n",
    "        for key, values in eval_od.items():\n",
    "            present_best_score = np.max(values)\n",
    "            \n",
    "        if present_best_score > best_score:\n",
    "            best_candidate = candidate\n",
    "            best_score = present_best_score\n",
    "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
    "                             (int(i + 1), int(n_models), present_best_score))\n",
    "\n",
    "        print('---------- ----------')\n",
    "            \n",
    "    return best_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ddd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of hyperparameter value lists for grid search\n",
    "gs_params = {'colsample_bytree': [0.3, 0.5, 0.7, 0.9],\n",
    "             'colsample_bylevel': [0.3, 0.5, 0.7, 0.9],\n",
    "             'learning_rate': [0.005, 0.05, 0.5],\n",
    "             'max_depth': [3, 5, 7], \n",
    "             'reg_alpha': [0.0005, 0.005, 0.05],\n",
    "             'reg_lambda': [0.0005, 0.005, 0.05],\n",
    "             'subsample': [0.3, 0.5, 0.7, 0.9],\n",
    "             'min_child_weight': [1, 5, 10], \n",
    "             'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4]}\n",
    "\n",
    "#define monotonicity constraints\n",
    "#train_corr_y = train[X_var + [y_var]].corr()[y_var].values[:-1]\n",
    "#train_corr_y[np.isnan(train_corr_y)] = 1\n",
    "#mono_constraints = tuple(int(i) for i in np.sign(train_corr_y))\n",
    "\n",
    "# start local timer\n",
    "mxgb_tic = time.time()\n",
    "\n",
    "# Monotonic XGBoost grid search                        \n",
    "best_mxgb = xgb_grid(train, valid, \n",
    "                     gs_params=gs_params, \n",
    "                     n_models=20, ntree=1000, \n",
    "                     early_stopping_rounds=100,\n",
    "                     verbose=False,\n",
    "                     seed=SEED)\n",
    "\n",
    "# end local timer\n",
    "mxgb_toc = time.time() - mxgb_tic\n",
    "print('Monotonic GBM training completed in %.2f s.' % (mxgb_toc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
